{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a14a8242",
   "metadata": {},
   "source": [
    "# This includes the set up snd Dataset preperation for the Falcon 40B model\n",
    "\n",
    "\n",
    "### 1. Understanding the Architecture\n",
    "Before we start coding, make sure you have the basics of the transformer architecture down. The Falcon 40B model is a large transformer model, and we will start by creating a simplified version of a transformer block.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "##### Padding and Truncation: \n",
    "Ensures sequences are of uniform length, which is critical for batch processing in neural networks.\n",
    "##### Tokenization: \n",
    "Converts text into a format that models can understand by mapping words or subwords to numerical IDs.\n",
    "##### Efficiency:\n",
    "Using batching and predefined padding/truncation helps manage computational resources effectively, especially when working with large datasets.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037ed205",
   "metadata": {},
   "source": [
    "## 2. Setting Up the Environment\n",
    "First, set up your environment. You can do this in a cloud environment like Google Colab, AWS, or your local machine with sufficient GPU capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72ada0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\swath\\anaconda3\\lib\\site-packages (2.4.1)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in c:\\users\\swath\\anaconda3\\lib\\site-packages (4.44.2)\n",
      "Requirement already satisfied: datasets in c:\\users\\swath\\anaconda3\\lib\\site-packages (2.21.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\swath\\anaconda3\\lib\\site-packages (from torch) (1.9)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\swath\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\swath\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\swath\\anaconda3\\lib\\site-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\swath\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\swath\\anaconda3\\lib\\site-packages (from torch) (2.6.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\swath\\anaconda3\\lib\\site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\swath\\anaconda3\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\swath\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\swath\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\swath\\anaconda3\\lib\\site-packages (from transformers) (1.22.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\swath\\anaconda3\\lib\\site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\swath\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\swath\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\swath\\anaconda3\\lib\\site-packages (from transformers) (0.24.6)\n",
      "Requirement already satisfied: xxhash in c:\\users\\swath\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\swath\\anaconda3\\lib\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\swath\\anaconda3\\lib\\site-packages (from datasets) (1.3.4)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\swath\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\swath\\anaconda3\\lib\\site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\swath\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\swath\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\swath\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\swath\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\swath\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\swath\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\swath\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\swath\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\swath\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\swath\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\swath\\anaconda3\\lib\\site-packages (from requests->transformers) (3.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\swath\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\swath\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\swath\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\swath\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\swath\\anaconda3\\lib\\site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\swath\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\swath\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\swath\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "pip install torch transformers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9d833d",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "For data preparation, we'll use the Hugging Face datasets library for simplicity. Here’s a basic setup for data loading:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12615335",
   "metadata": {},
   "source": [
    "This code snippet demonstrates how to load a dataset, tokenize its text data, and ensure that the tokenized sequences are properly padded and truncated using the Hugging Face datasets and transformers libraries. Let’s go through the code step by step:\n",
    "\n",
    "Note: If you are begginer go through these detailed steps with explanation of each. or skip to last.....\n",
    "\n",
    "### 1. Import Libraries:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9220ea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac4a813",
   "metadata": {},
   "source": [
    "##### load_dataset: \n",
    "A function from the Hugging Face datasets library that loads various datasets, including popular benchmark datasets like Wikitext.\n",
    "##### AutoTokenizer: \n",
    "A class from the Hugging Face transformers library that automatically selects the appropriate tokenizer based on the model specified.\n",
    "### 2. Load the Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "890f0d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d22e19",
   "metadata": {},
   "source": [
    "load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split='train'): Loads the Wikitext-2 dataset in raw format, specifically the training split.\n",
    "1. Wikitext-2: \n",
    "A dataset consisting of clean, formatted text from Wikipedia articles. It’s commonly used for training and\n",
    "evaluating language models.\n",
    "2. split='train': \n",
    "Specifies that only the training portion of the dataset should be loaded.\n",
    "\n",
    "### 3. Initialize the Tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d626f0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d031a2ea",
   "metadata": {},
   "source": [
    "AutoTokenizer.from_pretrained(\"gpt2\"): Loads the GPT-2 tokenizer, which is responsible for converting text into numerical representations (tokens) that the model can understand.\n",
    "\n",
    "### 4. Add a Custom Padding Token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52c48e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a custom pad token\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ed9a9c",
   "metadata": {},
   "source": [
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'}): Adds a custom padding token ([PAD]) to the tokenizer's vocabulary.\n",
    "Padding Token ([PAD]): A special token used to pad sequences to the same length, making it easier to process batches of data. This is important when dealing with variable-length sequences in machine learning models.\n",
    "\n",
    "### 5. Define the Tokenization Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1660a2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tokenization function\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize with padding and truncation\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=512)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e7049c",
   "metadata": {},
   "source": [
    "tokenize_function(examples): A custom function that takes a batch of examples and tokenizes them using the specified tokenizer settings.\n",
    "1. padding=\"max_length\": Pads all tokenized sequences to a fixed length (max_length), which is set to 512 tokens. This ensures all sequences in a batch are of the same length.\n",
    "2. truncation=True: Truncates sequences that exceed the maximum length, ensuring they fit within the defined size.\n",
    "3. max_length=512: Sets the maximum length for each tokenized sequence to 512 tokens.\n",
    "\n",
    "### 6. Apply the Tokenization Function to the Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a5f928b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d77faaa47aa544fd9e0cca0e2a00e434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply the tokenization function to the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4826f5e",
   "metadata": {},
   "source": [
    "1. dataset.map(tokenize_function, batched=True): Applies the tokenize_function to each example in the dataset in batches.\n",
    "2. batched=True: Allows the function to process multiple examples at once, improving performance and efficiency during tokenization.\n",
    "\n",
    "### 7. Print Tokenized Samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76a6a3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '', 'input_ids': [50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# Print some tokenized samples\n",
    "print(tokenized_datasets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad0b418",
   "metadata": {},
   "source": [
    "print(tokenized_datasets[0]): Displays the first tokenized example from the processed dataset.\n",
    "This output includes token IDs (numerical representations of the text), attention masks (indicating which tokens are padding), and any special tokens added (like [PAD]).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92b542f",
   "metadata": {},
   "source": [
    "# Full code in a single cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f08180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a sample dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split='train')\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Add a custom pad token\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Define the tokenization function\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize with padding and truncation\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Apply the tokenization function to the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Print some tokenized samples\n",
    "print(tokenized_datasets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91545ef6",
   "metadata": {},
   "source": [
    "### This code is an essential step in preparing data for training large language models like Falcon 40B, ensuring that the input data is formatted correctly for model consumption. Let me know if you need further clarification or assistance with any other part!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35e46db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
