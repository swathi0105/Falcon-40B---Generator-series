{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Falcon Model Architecture Implementation\n",
        "\n",
        "**Key Concepts:**\n",
        "\n",
        "\n",
        "1.  Transformer Architecture: Based on attention mechanisms, allowing the model to weigh different parts of the input differently, which is crucial for handling sequential data like text.\n",
        "2.   Model Customization: Parameters like n_embd, n_layer, and n_head can be adjusted to scale the model's capacity, which affects its performance and computational requirements.\n",
        "3.   Model Customization: Parameters like n_embd, n_layer, and n_head can be adjusted to scale the model's capacity, which affects its performance and computational requirements.\n",
        "4.   Last Hidden State: The output from the model that typically represents the processed information from the input sequence, which can be used for tasks like text generation or classification.\n",
        "This setup allows you to experiment with scaling up the model's architecture, similar to how Falcon 40B and other large language models are configured. If you have further questions or need more details on any part, let me know!\n",
        "\n",
        "Further the steps are classified as,\n",
        "\n",
        "*   **Define the Model**: Implement the Falcon 40B architecture in PyTorch. This\n",
        "involves creating the transformer blocks, defining the attention layers, and setting up the forward passes.\n",
        "*   **Scaling**: Ensure the architecture can handle billions of parameters and leverage efficient computation techniques like mixed-precision training and gradient checkpointing."
      ],
      "metadata": {
        "id": "qGX7Ow1vnI1B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Imports"
      ],
      "metadata": {
        "id": "-XJVh-I6oHD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import GPT2Model, GPT2Config\n"
      ],
      "metadata": {
        "id": "xuc_jIL_nlmt"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**torch** and **torch.nn**: Core libraries for building and training neural networks in PyTorch.\n",
        "\n",
        "**GPT2Model** and **GPT2Config**: Classes from the transformers library by Hugging Face, used to define and customize the GPT-2 model architecture."
      ],
      "metadata": {
        "id": "1DAzs2odnzXG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Defining the Falcon-like Model Class:"
      ],
      "metadata": {
        "id": "hZiJ7bAaobXl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FalconModel(nn.Module**): Defines a custom neural network class named FalconModel, inheriting from PyTorch's nn.Module class.\n",
        "__init__ **Method**: The constructor initializes the model\n",
        "\n",
        "\n",
        "1.   ***super(FalconModel, self)***.__init__(): Calls the parent class *(nn.Module)* initializer to set up the basic structure of the neural network.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WUgHyTU0osPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FalconModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FalconModel, self).__init__()\n",
        "        config = GPT2Config(\n",
        "            n_embd=1024,  # Number of embedding dimensions\n",
        "            n_layer=24,   # Number of transformer layers (blocks)\n",
        "            n_head=16     # Number of attention heads per transformer layer\n",
        "        )\n",
        "        self.transformer = GPT2Model(config)\n"
      ],
      "metadata": {
        "id": "ammcyaG4oFCr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.   **GPT2Config**: Creates a configuration object for the GPT-2 model, with the following customized parameters:\n",
        "\n",
        "  * n_embd=1024: Sets the size of the hidden layers or the embedding dimension to 1024. This value determines the size of the model's representation space.\n",
        "  *  n_layer=24: Specifies the number of transformer layers (or blocks). Increasing this value makes the model deeper, allowing it to learn more complex patterns.\n",
        "  *   n_head=16: Defines the number of attention heads per transformer layer. More heads allow the model to focus on different parts of the input simultaneously, enhancing its ability to capture various aspects of the context\n",
        "\n",
        "\n",
        "\n",
        "3.   **self.transformer = GPT2Model(config)**: Initializes a GPT-2 model using the specified configuration (config). This model is set as a module attribute of FalconModel."
      ],
      "metadata": {
        "id": "oAziPjd9rUEC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3**. **Forward** **Method**"
      ],
      "metadata": {
        "id": "wRDkW0dCrZLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, input_ids):\n",
        "    return self.transformer(input_ids).last_hidden_state"
      ],
      "metadata": {
        "id": "njovVecSqcOb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "forward(self, input_ids): Defines how the data passes through the model during training or inference.\n",
        "\n",
        "\n",
        "*   **input_ids**: The input tensor containing token IDs (numerical representations of text).\n",
        "*  **self.transformer(input_ids)**: Passes the input through the GPT-2 model, which processes the input IDs through its layers to produce an output.  \n",
        "*   **last_hidden_state**: Extracts the last hidden states from the GPT-2 output, which represent the contextualized embeddings of the input tokens.\n",
        "\n"
      ],
      "metadata": {
        "id": "RCuVR3ntofVS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model Instantiation and Summary"
      ],
      "metadata": {
        "id": "vgvseNlhqGga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "model = FalconModel()\n",
        "\n",
        "# Check model summary\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "OLKzjtpLp1fy",
        "outputId": "8b7e1c3d-1ca6-42ca-8c8a-694841ddf753",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FalconModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 1024)\n",
            "    (wpe): Embedding(1024, 1024)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-23): 24 x GPT2Block(\n",
            "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2SdpaAttention(\n",
            "          (c_attn): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "*   **model = FalconModel():** Creates an instance of the FalconModel.\n",
        "*   **print(model):** Prints the model's structure, showing all layers and components within the model, which helps verify that the architecture matches the intended configuration.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YQaqoSGXqzuj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note: This note book provides only the model development. **"
      ],
      "metadata": {
        "id": "c2CG-X3vqz2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KebE9kAjvThr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}