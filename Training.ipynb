{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Model\n",
        "*  Training Pipeline: Set up a training loop that includes forward and backward passes, optimization, and checkpointing.\n",
        "*Optimization: Use optimizers suitable for large-scale training like AdamW. Consider learning rate scheduling and other tricks to stabilize training.\n",
        "\n",
        "**Key Points:**\n",
        "* **Data Loading and Batching**: Efficiently handles large datasets and allows for training in smaller, manageable pieces (batches).\n",
        "* **Optimization:** Uses the AdamW optimizer, which is well-suited for training large transformer models due to its adaptive learning rate and weight decay to prevent overfitting.\n",
        "* **Device Management**: Efficiently uses available hardware resources (GPU if available) for faster training.\n",
        "* **Training Loop:** Iteratively processes data, computes loss, and updates model weights to minimize the loss over time."
      ],
      "metadata": {
        "id": "myaeahml_aB9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Imports"
      ],
      "metadata": {
        "id": "5ueB29eHBPiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW\n"
      ],
      "metadata": {
        "id": "GNLn3BQP_fGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **DataLoader**: A PyTorch utility that provides an easy way to iterate over datasets, supporting batching, shuffling, and parallel data loading.\n",
        "* **AdamW**: An optimizer from the Hugging Face Transformers library that implements the Adam algorithm with weight decay, which is commonly used for training transformer-based models."
      ],
      "metadata": {
        "id": "g7B1k17YBgzj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Setting Up the DataLoader"
      ],
      "metadata": {
        "id": "0qFIBLahBtLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simplified DataLoader\n",
        "train_dataloader = DataLoader(tokenized_datasets, batch_size=8, shuffle=True)"
      ],
      "metadata": {
        "id": "D7tydUuEBf2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DataLoader(tokenized_datasets, batch_size=8, shuffle=True):\n",
        "* **tokenized_datasets:** The dataset that was tokenized in previous steps.\n",
        "* **batch_size=8:** Specifies that the data will be loaded in batches of 8 samples at a time.\n",
        "* **shuffle=True:** Randomly shuffles the data at every epoch to help the model generalize better."
      ],
      "metadata": {
        "id": "e7YxrCzLCUbL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Define the optimizer"
      ],
      "metadata": {
        "id": "ePOah_8ZCUei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)"
      ],
      "metadata": {
        "id": "frqKGvVUCTnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AdamW(model.parameters(), lr=5e-5):\n",
        "*  **model.parameters()** : Retrieves the parameters of the model that need to be optimized.\n",
        "* **lr=5e-5:** Sets the learning rate, which controls how much the model weights are adjusted with each update. A learning rate of 5e-5 is a common starting point for fine-tuning large transformer models."
      ],
      "metadata": {
        "id": "TB3DQgUuC6V0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Setting up the device"
      ],
      "metadata": {
        "id": "LIAV0wWEDNE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "id": "z83O2umvCTqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "* Checks if a GPU (CUDA) is available. If so, it uses the GPU for faster computation; otherwise, it defaults to the CPU.\n",
        "\n",
        "model.to(device): Moves the model to the specified device (GPU or CPU)."
      ],
      "metadata": {
        "id": "wL91jCdmDg47"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Training Loop"
      ],
      "metadata": {
        "id": "-YasPSlDD6JG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(3):  # Example: 3 epochs\n",
        "    for batch in train_dataloader:\n",
        "        inputs = batch['input_ids'].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = outputs.loss  # Replace with actual loss computation\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch} completed with loss: {loss.item()}\")\n"
      ],
      "metadata": {
        "id": "H0xLEa6fDw2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Looping Over Epochs:**\n",
        "* for epoch in range(3): Loops over the dataset for a fixed number of epochs (in this example, 3 epochs). An epoch is a complete pass through the entire training dataset.\n",
        "\n",
        "**Batch Processing:**\n",
        "* for batch in train_dataloader: Iterates over batches of data loaded by the DataLoader.\n",
        "inputs = batch['input_ids'].to(device): Extracts the input IDs from the batch and moves them to the specified device (GPU or CPU).\n",
        "\n",
        "**Forward Pass:**\n",
        "* outputs = model(inputs): Feeds the input data through the model to get the outputs.\n",
        "* loss = outputs.loss: Attempts to access the loss from the model outputs, though this line assumes that the model output includes a loss attribute, which is not directly available with GPT-2. This should be replaced with actual loss computation code, such as:"
      ],
      "metadata": {
        "id": "tCqgJcksEW3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of actual loss computation (replace the above line)\n",
        "loss_fn = nn.CrossEntropyLoss()  # Define the loss function\n",
        "logits = outputs  # Model outputs (logits)\n",
        "labels = inputs  # Example: using inputs as labels for self-supervised learning\n",
        "loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n"
      ],
      "metadata": {
        "id": "6YCMIAjyE39Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Backward Pass:**\n",
        "* optimizer.zero_grad(): Clears the gradients of all optimized parameters. This is important to prevent accumulation of gradients from multiple backward passes.\n",
        "* loss.backward(): Computes the gradient of the loss with respect to the model parameters using backpropagation.\n",
        "* optimizer.step(): Updates the model parameters based on the computed gradients.\n",
        "\n",
        "**Print Loss:**\n",
        "* print(f\"Epoch {epoch} completed with loss: {loss.item()}\"): Prints the loss value at the end of each epoch to track training progress."
      ],
      "metadata": {
        "id": "OfCVumsSE_9g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note : This notebook is explained with each steps involved not including any outputs for all**\n",
        "\n",
        "Refer to https://github.com/swathi0105/Falcon-40B---Generator-series\n",
        "Goto -> Falcon 40B.ipynb file"
      ],
      "metadata": {
        "id": "_hIMj6VdFXJo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LL5oALY4F_v9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}